{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers in between the input and output layer are called hidden layers. There is no special meaning to this phrase; it simply indicates that these neurons are performing intermediate calculations. __Deep Learning__ is a loosely defined term which implies that many hidden layers are being used.\n",
    "\n",
    "The general setup consider $L$ layers, with layers 1 and $L$ being the input and output layers, respectiveley. Suppose that layer $l$, for $l=1, 2, \\ldots, L$ contains $n_l$ neurons. So $n_1$ is the dimension of the input data.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{[1]} &= x \\\\\n",
    "a^{[l]} &= \\sigma \\left( W^{[l]} a^{[l-1]} + b^{[l]} \\right) \\quad \\text{for } l=2, 3, \\ldots, L.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now suppose we have $N$ samples of training data,\n",
    "$$\n",
    "\\left\\{ x^{\\{i\\}} \\right\\}_{i=1}^N \\subset \\mathbb{R}^{n_1}\n",
    "$$\n",
    "for which there are given target outputs\n",
    "$$\n",
    "\\left\\{ y^{\\{i\\}} \\right\\}_{i=1}^N \\subset \\mathbb{R}^{n_L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a classification task with the same MNIST dataset but using PyTorch. Nowadyays, there are many Neural Network / Deep Learning frameworks availables, such as Tensorflow, Keras, JAX, Matlab, etc. We decided to use PyTorch since it has a really friendly user API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need the data. The way to access to it is different to scikit-learn, but it is essentialy the same but we load train and test sets in a separate way. Also, since the elements are images it is necessary to cast them to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some samples again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` is a tool for handle batches and iterate over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the size of each batch, in particular `X`-batches are made of only one channel and 28 $\\times$ 28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating our model we need to select the device where PyTorch is going to work on. By default tensors are created on the CPU, but with the following line we can use the GPU (if it is available).\n",
    "\n",
    "__Remark:__ If you running this notebook on Google Colab you need to enable GPU manually. Go to Menu `Menu > Runtime > Change Runtime` and change hardware accelaration to __GPU__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a fully connected neural network made of three layers (just one hidden layer). Note the activation function is not a logistic one, but another non-linear function calle _ReLU_ (Rectified Linear Unit) defined as \n",
    "\n",
    "$$\n",
    "ReLu(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "![ReLU](../images/ReLU.png)\n",
    "\n",
    "[Source](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic cost function that we widh to minimize hast the form\n",
    "\n",
    "$$\n",
    "J\\left(W, b\\right) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left\\lVert y^{\\{i\\}} - a^{[L]} \\left(x^{\\{i\\}}\\right) \\right\\rVert^2_2\n",
    "$$\n",
    "\n",
    "where $W$, $b$ are the set of all the weight matrices and biases vectors, respectively.\n",
    "\n",
    "We can minimize the cost function with gradient descent as we already saw previously. Without loss of generality, consider $J: \\mathbb{R}^s \\to \\mathbb{R}$ and $\\theta \\in \\mathbb{R}^s$, then the algorithm has the following form\n",
    "\n",
    "$$\n",
    "\\theta \\to \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and the partial derivative of the cost function is a sum over the training data of individual partial derivatives, more precisely\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla C_{x^{\\{i\\}}}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to choose a loss (cost) function and the optimizer. \n",
    "\n",
    "- Cross Entropy is useful when training a classification problem with several classes. In this cases, 10 digits.\n",
    "- Stochastic Gradient Descent with a learning rate of 0.001 (it is a standard practice to use this value as first guest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a large number of parameters and a large number of training points, computing the gradient vector at every iteration of the steepest descent method can be prohibitively expensive. A cheaper alternative is __*Stochastic Gradient Descent*__ which has several variations, one of the most commons is to take a smaller set of training data in each iteration. For some $m \\ll N$ each iteration has the following form:\n",
    "\n",
    "1. Choose $m$ integers, $k_1, k_2, \\ldots, k_m$ uniformly at random from $\\{1, 2, \\ldots, N\\}$.\n",
    "2. Update \n",
    "$$\n",
    "\\theta \\to \\theta - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\nabla C_{x^{\\{k_i\\}}}(\\theta)\n",
    "$$\n",
    "\n",
    "In this iteration, the set $\\{ x^{\\{k_i\\}} \\}_{i=1}^k$ is known as a _minibatch_. There is a without replacement alternative where, assuming $N = Km$ for some $K$, we split the training set randomly into $K$ distinct minibatches and cycle through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to compute partial derivatives of the cost function with respect to each element of $W^{[l]}$ and $b^{[l]}$. We therefore focus our attention on computing those individual partial derivatives.\n",
    "\n",
    "Hence, for a fixed $i$-th training sample we want to compute the derivatives of $C_{x^{\\{i\\}}}$, so we may drop the dependence on $x^{\\{i\\}}$ and simply write\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2} \\left\\lVert y - a^{[L]}\\right\\rVert_2^2\n",
    "$$\n",
    "\n",
    "It is useful introduce two further sets of variables. First we let\n",
    "$$\n",
    "z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\in \\mathbb{R}^{n_l} \\quad \\text{for} \\; l=2, 3, \\ldots, L.\n",
    "$$\n",
    "where $z_j^{[l]}$, the $j$-th component of $z_j^{[l]}$ is the _weighted input_ for neuron $j$ at layer $l$.\n",
    "\n",
    "Secondly,\n",
    "$$\n",
    "\\delta_j^{[l]} = \\dfrac{\\partial C}{\\partial z_j^{[l]}} \\in \\mathbb{R}^{n_l} \\quad \\text{for} \\; 1 \\leq j \\leq n_l \\quad \\text{and} \\quad 2 \\leq l \\leq L\n",
    "$$\n",
    "\n",
    "The output $a^{[L]}$ can be evaluated from a forward pass through the network, computing in order\n",
    "$$\n",
    "a^{[1]}, z^{[2]}, a^{[2]}, z^{[3]}, a^{[3]}, \\ldots, z^{[L]}, a^{[L]}\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\delta^{[L-1]}, \\delta^{[L-2]}, \\ldots, \\delta^{[2]}\n",
    "$$\n",
    "can be computed in backward pass such that \n",
    "$$\n",
    "\\dfrac{\\partial C}{\\partial W^{[l]}_{j, k}} = \\delta_j^{[l]} a_k^{[l-1]}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\dfrac{\\partial C}{\\partial \\beta^{[l]}_j} = \\delta^{[l]}_j\n",
    "\\quad\n",
    "\\text{for} \\; 1 \\leq j, k \\leq n_l \\quad \\text{and} \\quad 2 \\leq l \\leq L\n",
    "$$\n",
    "\n",
    "Computing gradients in this way is known as __*back propagation*__.\n",
    "\n",
    "Another good resource for understand back propagation is [this website](https://colah.github.io/posts/2015-08-Backprop/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to decide the number of _epochs_ and just train the data. Sometimes epochs and batch size confuse people.\n",
    "\n",
    "- The batch size is a hyperparameter of gradient descent that controls the number of __training samples__ to work through before the model’s internal parameters are updated.\n",
    "- The number of epochs is a hyperparameter of gradient descent that controls the number of __complete passes through the training dataset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.307937  [    0/60000]\n",
      "loss: 2.306705  [ 6400/60000]\n",
      "loss: 2.298175  [12800/60000]\n",
      "loss: 2.282872  [19200/60000]\n",
      "loss: 2.289108  [25600/60000]\n",
      "loss: 2.291603  [32000/60000]\n",
      "loss: 2.272025  [38400/60000]\n",
      "loss: 2.273087  [44800/60000]\n",
      "loss: 2.266502  [51200/60000]\n",
      "loss: 2.256920  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.259253 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.260177  [    0/60000]\n",
      "loss: 2.256225  [ 6400/60000]\n",
      "loss: 2.255603  [12800/60000]\n",
      "loss: 2.222483  [19200/60000]\n",
      "loss: 2.240228  [25600/60000]\n",
      "loss: 2.242202  [32000/60000]\n",
      "loss: 2.213277  [38400/60000]\n",
      "loss: 2.229506  [44800/60000]\n",
      "loss: 2.207494  [51200/60000]\n",
      "loss: 2.191150  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 2.196347 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.197476  [    0/60000]\n",
      "loss: 2.187516  [ 6400/60000]\n",
      "loss: 2.193516  [12800/60000]\n",
      "loss: 2.134229  [19200/60000]\n",
      "loss: 2.166577  [25600/60000]\n",
      "loss: 2.165516  [32000/60000]\n",
      "loss: 2.121250  [38400/60000]\n",
      "loss: 2.155981  [44800/60000]\n",
      "loss: 2.112214  [51200/60000]\n",
      "loss: 2.082825  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 2.091023 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.093355  [    0/60000]\n",
      "loss: 2.070872  [ 6400/60000]\n",
      "loss: 2.087190  [12800/60000]\n",
      "loss: 1.984253  [19200/60000]\n",
      "loss: 2.037401  [25600/60000]\n",
      "loss: 2.029406  [32000/60000]\n",
      "loss: 1.961776  [38400/60000]\n",
      "loss: 2.020380  [44800/60000]\n",
      "loss: 1.945934  [51200/60000]\n",
      "loss: 1.896134  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 1.905321 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.911534  [    0/60000]\n",
      "loss: 1.865433  [ 6400/60000]\n",
      "loss: 1.898744  [12800/60000]\n",
      "loss: 1.740466  [19200/60000]\n",
      "loss: 1.810979  [25600/60000]\n",
      "loss: 1.791669  [32000/60000]\n",
      "loss: 1.702192  [38400/60000]\n",
      "loss: 1.792895  [44800/60000]\n",
      "loss: 1.684868  [51200/60000]\n",
      "loss: 1.614039  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.617551 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "- [Deep Learning: An Introduction for Applied Mathematicians](https://epubs.siam.org/doi/10.1137/18M1165748)\n",
    "- [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pinn-backup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35923595c379b2f8a43fe857c5f1ff97e2ec11b937ce0326cd8f57467725d828"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
